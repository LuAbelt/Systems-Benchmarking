\documentclass[	runningheads,
				%deutsch, % Tell llncs that the keywords should be in german
				%german,  % Needed for the \ifgerman-command
				a4paper]{llncs}

\usepackage{url}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{hyperref}

% Support for special characters like "Umlaute"
\usepackage[utf8]{inputenc}

\usepackage[english]{babel}

\usepackage{glossaries}

\makeglossaries

\loadglsentries{glossary}
%*********************************************************************%
% META                                                                %
%*********************************************************************%
\newcommand{\university}{Saarland University}
\newcommand{\school}{Saarland Informatics Campus}
\newcommand{\thetitle}{Seminar: Systems Benchmarking}
\newcommand{\shorttitle}{Seminar: Systems Benchmarking}
\newcommand{\thedate}{\today{}}
\newcommand{\thegermandate}{15. April}

\newcommand{\theforename}{Lukas}
\newcommand{\thesurname}{Abelt}

% Advisors
	\newcommand{\advisor}{Advisors}
	\newcommand{\advisors}{Prof. Sven Apel, \\ Christian Hechtl}

% Title for the seminar
	%\newcommand{\theseminartitle}{"Performance Measurements Before Releases vs. Each Commits"}
	\newcommand{\theseminartitle}{"Using Benchmarking in Productive Development Systems -- Opportunities and Challenges"}

%*********************************************************************%
% THE DOCUMENT                                                        %
%*********************************************************************%

\begin{document}
	%*********************************************************************%
	% TITLE                                                               %
	%*********************************************************************%
	
	% Arabic page numbering
	\mainmatter 
		
	% Title including a subtitle for the name of the seminar
	\title{\theseminartitle \\ \small \thetitle}
	
	% (Optional) In the case that the initial title is too long, the short title will be used
	%\titlerunning{Hauptseminar: Human and Social Factors in Software Engineering}
	
	\author{\theforename\ \thesurname \small \\ \ \\ \advisor : \ \advisors}
	
	% (Optional) This will appear near the page number
	\authorrunning{\shorttitle}
	
	\institute{\school ,\\ \university}
	
	\maketitle
	
	%*********************************************************************%
	% CONTENT                                                             %
	%*********************************************************************%

% General Structure
	% 1. Introduction and Motivation
	% 2. General Definitions
	%		2.1 What is a Benchmark
	%		2.2 What is a continuous Benchmark
	% 3. Experiment
	%		3.1 Experiment Idea&Goals
	%		3.2 Setup
	%		3.3 Evaluation
	%	4. 

	% Introduction
\section{Introduction}
Over the past years system benchmarks have become more prominent in a variety of contexts and use cases. As a result more benchmarking tools became available that are used in both commercial systems and are widely adopted for academic purposes.

In this paper we give an overview of the general concepts of system benchmarks before we consider in detail how classical benchmarking approaches can be used and extended to integrate it into the standard development process of software. Specifically, we take a look at how benchmark can be used to swiftly detect and react to performance changes throughout the development process. In order to do this we will take a look at Change-Point detection algorithms. We will outline and analyse the methodology and opportunities of such approaches while taking a careful look at the challenges that may arise due to this.

To support and evaluate our claims, we will conduct a practical experiment where we will use and compare several banchmarking techniques on a piece of well known, open-source software.
TODO

\section{System Benchmarks}
\label{sec:benchmarking}
The term "Benchmark" is a broad term that can be interpeted and defined in many different ways depending on the context and application domain. We will use this chapter to give a definition of Benchmarking how we will use it throughout this paper.

\subsection{Definition}
\label{ssec:bench_definition}
The term "Benchmark" is used in a variaty of different domains and, according to the Standard Specialization Evalutation Corporation, historically stems from a physical marking that was used as a refernce point on a workbench to check the length of the produced pieces\footnote{SPEC Glossary: \url{https://www.spec.org/spec/glossary/\#benchmark}}. However throughout this paper we are only interested in system benchmarks within the context of software systems and will therefore also refer to the definition of a benchmark as given by Kounev et al.: "a tool coupled with a methodology for the evaluation and comparison of systems or components with respect to specific characteristics, such as performance, reliability or security." (\cite{Kounev} p. 4). As already given from this definition benchmarks can serve as an evaluation for different characteristics. We will more closely look at these in \autoref{ssec:bench_classification}. Throughout this paper we will also mostly refer to the "system or component" that is being benachmarked as the \gls{sut}.

\subsection{Classification of Benchmarks}
\label{ssec:bench_classification}
Benchmarks can be classified in a variety of different means. In this section we will give a small overview of the different types of benchmarks there are with respect to the two following questions:
\begin{enumerate}
	\item What overall goal does a benchmark serve?
	\item What qualities are evaluated by an benchmark?
	\item How can a benchmark be performed (Benchmarking strategies)
\end{enumerate}

In the broadest sense, benchmarks can be divided into competetive and non-competetive benchmarks. The main motivation of competetive benchmarks is hereby the development of standardized quality criteria that can be used to compare different systems. Certain sources also emphasize this importance on the competetiveness by providing a more detailled definition of the term benchmark, i.e. "a standard tool for competetive evaluation and comparison of competetive systems" (\cite{kistowski2015} p. X).

For non-competetive benchmarks Kounev et al. Further distiguish between the two categories of "rating tools" and "research benchmark". Rating tools can serve multiple purposes: Either as a component of an development and system improvement process, as a baseline for regulatory programmes or to serve as common benchmark for research. Research benchmarks on the other hand commonly refer to tools and workloads that may be used for in-depth evaluation of research results, prototypes or productive systems. Based on these descriptions it becomes apparent that there sometimes might be no clear distinction between a rating tool and a research benchmark and the terminology is often determined by the evaluating party (See \cite{Kounev} p. 4).

Regardless of the competetiveness, or lack thereof, of a benchmark the key metrics that are evaluated are often similar or even identical. While historically the main metrics of importance were e.g. security, system reliability and energy efficiency, modern benchmarks also try to cover much more aspects of a software. We will discuss a selection of the quality attributes listed by Kounev et al. that are usually target of modern benchmarks(\cite{Kounev} p. 6 f):

\paragraph{Performance} The term Performance itself can be interpreted in many different means. For this paper, we will focus on one of the most basic interpretations: The amount of work done compared to the time and resources used. A much more in-depth explanation of different performance measures is given by Kounev et al. (See \cite{Kounev} p. 49 ff)

\paragraph{Scalability} TODO Definition needed

\paragraph{Energy Efficiency} TODO Definition needed

\paragraph{Reliability} TODO Definition needed

When it comes to benchmarking strategies two approaches are most prominent: Fixed-Time and Fixed-Work benchmarks. A Fixed-Work Benchmark here describes the most intuitive approach for benchmarking: We fix a specified amount of work $W_i$ and measure the required time $T_i$ that is required to fulfill this work.

While Fixed-Work benchmarks are easy to understand and implement, it introduces an implicit performance bottleneck that is also known as \textit{Amdahl's Law}. It states that the maximum possible performance improvement that can be achieved in Fixed-Work Benchmarks is prohibitly upper-bounded by the fraction of work that is performed by the component that is being improved (See \cite{Kounev} p. 9f).

In Fixed-Time benchmarks, instead of fixing the amount of work $W_i$, the time $T_i$ is bound and the amount of work finished in this time is measured as a metric. As opposed to Fixed-Work Benchmarks the Fixed-Time Benchmarks do not suffer the issue of an upper-bounded maximum improvement as for every improvement, the reduced time can be used to process further work units (See \cite{Kounev} p. 11).

Additionally there are also further approaches that does not employ fixed time or work at all, but rather employ variable-time and -work characteristics. For this case the matric is a function of the time $T$ and the work $W$. While this technique provides maximum flexibility, this approach is not suitable for all types of benchmarks (See \cite{Kounev} p. 12)

\subsection{Evaluating Benchmarks}

Apart from the metrics that evaluate the result of a benchmark, there are also several attributes that are important to evaluate the quality of the benchmark itself. The designer of a benchmark should always take these attributes into consideration when creating a benchmark. As described by Skadron et al. it is not possible to create a single benchmark that covers all these attributes perfectly. Therefore one either has to create mutliple benchmarks that complement each others weaknesses or choose to sacrifice one of these attributes over the other (\cite{Skadron2003} p. 32f).

Over the years, researchers and industry associates alike have defined several criteria that are considered desirable by modern standards. Often the exact terminology of these criteria differs based on authors and the specific application domain, however the concepts in general are similar. Kounev at al. list five core criteria that will be defined in a little more detail in this chapter (\cite{Kounev} p. 13f). A much more in depth discussion of these criteria was performed by Kistowski et al. (See \cite{kistowski2015})

\paragraph{Relevance} For a benchmark to be meaningful it is important that it, at least to a certain extent, reproduces some scenarios with real world application. This is what the "Relevance" of a benchmark refers to. It tries to define how relevant the produced information is to a potential user of a software. According to Kistowski et al. this makes it "perhaps the most important characteristic" (\cite{kistowski2015} p. 334). However the relevance of a benchmark may also limits the benchmarks applicability meaning, that a highly relevant benchmark for a specific system or component might only narrow applicability while benchmarks with a broad applicability will have a lower relevance but for a wider selection of applications. Kitowski et al. further elaborate on this with specific examples (\cite{kitowski2015} p. 334f)

\paragraph{Reproducibility} Reproducability ensures that it is possible for a third party to produce ideally the same results for a given test environment. Reproducability covers both the consistency between mutliple runs on the same system as well as creating the same results on another environment with a semantically equivalent configuration. 

However, with modern hard- and software systems, optimal reproducability has become challenging. For both hard- and software there are several factors that induce variability. From a hardware side things such as the physical disc layout or network connections may affect benchmarking results. For software systems the causes for variability stem from various different sources. from a low-level perspective, things such as thread scheduling can influence the benchmark results. From a higher level perspective a benchmark has to consider the configurability of a software system itself. Modern software systems often offer lots of different configuration options which can be used to create a tailor made system-variant. En- or disabling a specific feature might heavily influence the performance of a benchmark.

According to Huppler, an ideal benchmark result should be definable as a function fo a hard- and software configuration (See \cite{huppler2009} p. 335). However for real-world applications, modelling such a function is far too complex which requires the benchmark designer to try to model reproducability in other ways. In the end the reproducability of a benchmark is heavuly dependent on the ability to reproduce the benchamrking environment that was used to create this specific benchmark. 

\paragraph{Fairness} TODO: Short explanation
\paragraph{Verifiability} TODO: Short explanation
\paragraph{Usability} TODO: Short explanation

\section{Detecting Performance Changes}



\section{Continuous Benchmarking}
\subsection{Definition}
\subsection{Strategies and Opportunities}
\subsection{Challenges}

\section{Experiment}
In this chapter we will further elaborate on the experiment that was conducted. The chapter will give an overview over the general experiment idea and it's goals, the experiment setup and the specific results that were achieved in the experiment runs. An in-depth analysis and evaluation of the experiment results is performed in \autoref{sec:exp_evaluation}

	\subsection{Experiment Idea \& Goals}
	The main idea of the experiment that was created was to perform a benchmark on a series of versions of a specific open-source software to evaluate the methods and techniques described in this paper. More specifically, we want to evaluate the following questions:
	\begin{itemize}
		\item When do we need to benchmark software?
			\begin{itemize}
				\item Is it sufficient to benchmark releases?
				\item Do we need to benchmark every commit?
			\end{itemize}
		\item How do different continuous benchmarking approaches compare with respect to:
			\begin{itemize}
				\item Complexity of implementation?
				\item Reliably detecting performance changes?
				\item Reducing the amount of manual work required by the developer?
			\end{itemize}
	\end{itemize}

	Additionally the experiment should also be used to identify other, currently unidentified, challenges and opportunities that this approach might offer.

	For the first complex of the questions mentioned above, the follwoing methology is planned: Firstly, perform a benchmark of all public releases on the open-source software that was selected. In a second step then every individual commit is benchmarked for more fine-grained data. By comparing the performance results of the pure release benchmarking and the individual commit benchmarking it is eventually possible to decide if a specific performance regression could have been detected earlier if a continuous benchmarking approach would have been used.

	\subsection{Experiment Setup}


	\subsection{Experiment Results}

\section{Evaluation of Performance Results}
\label{sec:exp_evaluation}
\section{Reflection}

\section{Future Work}
	
	%*********************************************************************%
	% APPENDIX                                                            %
	%*********************************************************************%
	
	% Insert the appendix here. You can alternatively include files via: \include{pathToFile}
	
	%*********************************************************************%
	% LITERATURE                                                          %
	%*********************************************************************%
	% As a recommendation JabRef might be a usefull tool for this section. Use myRefs.bib therefore
	\phantomsection
	\bibliographystyle{splncs03}
	\bibliography{literature}
\end{document}
