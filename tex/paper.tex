\documentclass[	runningheads,
				%deutsch, % Tell llncs that the keywords should be in german
				%german,  % Needed for the \ifgerman-command
				a4paper]{llncs}

\usepackage{url}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{hyperref}

% Support for special characters like "Umlaute"
\usepackage[utf8]{inputenc}

\usepackage[english]{babel}

\usepackage{glossaries}

\makeglossaries

\loadglsentries{glossary}
%*********************************************************************%
% META                                                                %
%*********************************************************************%
\newcommand{\university}{Saarland University}
\newcommand{\school}{Saarland Informatics Campus}
\newcommand{\thetitle}{Seminar: Systems Benchmarking}
\newcommand{\shorttitle}{Seminar: Systems Benchmarking}
\newcommand{\thedate}{April 15}
\newcommand{\thegermandate}{15. April}

\newcommand{\theforename}{Lukas}
\newcommand{\thesurname}{Abelt}

% Advisors
	\newcommand{\advisor}{Advisors}
	\newcommand{\advisors}{Prof. Sven Apel, \\ Christian Hechtl}

% Title for the seminar
	%\newcommand{\theseminartitle}{"Performance Measurements Before Releases vs. Each Commits"}
	\newcommand{\theseminartitle}{"Using Benchmarking in Productive Development Systems -- Opportunities and Challenges"}

%*********************************************************************%
% THE DOCUMENT                                                        %
%*********************************************************************%

\begin{document}
	%*********************************************************************%
	% TITLE                                                               %
	%*********************************************************************%
	
	% Arabic page numbering
	\mainmatter 
		
	% Title including a subtitle for the name of the seminar
	\title{\theseminartitle \\ \small \thetitle}
	
	% (Optional) In the case that the initial title is too long, the short title will be used
	%\titlerunning{Hauptseminar: Human and Social Factors in Software Engineering}
	
	\author{\theforename\ \thesurname \small \\ \ \\ \advisor : \ \advisors}
	
	% (Optional) This will appear near the page number
	\authorrunning{\shorttitle}
	
	\institute{\school ,\\ \university}
	
	\maketitle
	
	%*********************************************************************%
	% CONTENT                                                             %
	%*********************************************************************%

% General Structure
	% 1. Introduction and Motivation
	% 2. General Definitions
	%		2.1 What is a Benchmark
	%		2.2 What is a continuous Benchmark
	% 3. Experiment
	%		3.1 Experiment Idea&Goals
	%		3.2 Setup
	%		3.3 Evaluation
	%	4. 

	% Introduction
\section{Introduction}
Over the past years system benchmarks have become more prominent in a variety of contexts and use cases. As a result more benchmarking tools became available that are used in both commercial systems and are widely adopted for academic purposes.

In this paper we give an overview of the general concepts of system benchmarks before we consider in detail how classical benchmarking approaches can be used and extended to integrate it into the standard development process of software. Specifically, we take a look at how benchmark can be used to swiftly detect and react to performance changes throughout the development process. In order to do this we will take a look at Change-Point detection algorithms. We will outline and analyse the methodology and opportunities of such approaches while taking a careful look at the challenges that may arise due to this.

To support and evaluate our claims, we will conduct a practical experiment where we will use and compare several banchmarking techniques on a piece of well known, open-source software.
TODO

\section{System Benchmarks}
\label{sec:benchmarking}
The term "Benchmark" is a broad term that can be interpeted and defined in many different ways depending on the context and application domain. We will use this chapter to give a definition of Benchmarking how we will use it throughout this paper.

\subsection{Definition}
\label{ssec:bench_definition}
The term "Benchmark" is used in a variaty of different domains and, according to the Standard Specialization Evalutation Corporation, historically stems from a physical marking that was used as a refernce point on a workbench to check the length of the produced pieces\footnote{SPEC Glossary: \url{https://www.spec.org/spec/glossary/\#benchmark}}. However throughout this paper we are only interested in system benchmarks within the context of software systems and will therefore also refer to the definition of a benchmark as given by Kounev et al.: "a tool coupled with a methodology for the evaluation and comparison of systems or components with respect to specific characteristics, such as performance, reliability or security." (\cite{Kounev} p. 4). As already given from this definition benchmarks can serve as an evaluation for different characteristics. We will more closely look at these in \autoref{ssec:bench_classification}. Throughout this paper we will also mostly refer to the "system or component" that is being benachmarked as the \gls{sut}.

\subsection{Classification of Benchmarks}
\label{ssec:bench_classification}
Benchmarks can be classified in a variety of different means. In this section we will give a small overview of the different types of benchmarks there are with respect to the two following questions:
\begin{enumerate}
	\item What overall goal does a benchmark serve?
	\item What qualities are evaluated by an benchmark?
	\item How can a benchmark be performed (Benchmarking strategies)
\end{enumerate}

In the broadest sense, benchmarks can be divided into competetive and non-competetive benchmarks. The main motivation of competetive benchmarks is hereby the development of standardized quality criteria that can be used to compare different systems. Certain sources also emphasize this importance on the competetiveness by providing a more detailled definition of the term benchmark, i.e. "a standard tool for competetive evaluation and comparison of competetive systems" (\cite{kistowski2015} p. X).

For non-competetive benchmarks Kounev et al. Further distiguish between the two categories of "rating tools" and "research benchmark". Rating tools can serve multiple purposes: Either as a component of an development and system improvement process, as a baseline for regulatory programmes or to serve as common benchmark for research. Research benchmarks on the other hand commonly refer to tools and workloads that may be used for in-depth evaluation of research results, prototypes or productive systems. Based on these descriptions it becomes apparent that there sometimes might be no clear distinction between a rating tool and a research benchmark and the terminology is often determined by the evaluating party (See \cite{Kounev} p. 4).

Regardless of the competetiveness, or lack thereof, of a benchmark the key metrics that are evaluated are often similar or even identical. While historically the main metrics of importance were e.g. security, system reliability and energy efficiency, modern benchmarks also try to cover much more aspects of a software. We will discuss a selection of the quality attributes listed by Kounev et al. that are usually target of modern benchmarks(\cite{Kounev} p. 6 f):

\paragraph{Performance} The term Performance itself can be interpreted in many different means. For this paper, we will focus on one of the most basic interpretations: The amount of work done compared to the time and resources used. A much more in-depth explanation of different performance measures is given by Kounev et al. (See \cite{Kounev} p. 49 ff)

\paragraph{Scalability} TODO Definition needed

\paragraph{Energy Efficiency} TODO Definition needed

\paragraph{Reliability} TODO Definition needed

When it comes to benchmarking strategies two approaches are most prominent: Fixed-Time and Fixed-Work benchmarks. A Fixed-Work Benchmark here describes the most intuitive approach for benchmarking: We fix a specified amount of work $W_i$ and measure the required time $T_i$ that is required to fulfill this work.

While Fixed-Work benchmarks are easy to understand and implement, it introduces an implicit performance bottleneck that is also known as \textit{Amdahl's Law}. It states that the maximum possible performance improvement that can be achieved in Fixed-Work Benchmarks is prohibitly upper-bounded by the fraction of work that is performed by the component that is being improved (See \cite{Kounev} p. 9f).

In Fixed-Time benchmarks, instead of fixing the amount of work $W_i$, the time $T_i$ is bound and the amount of work finished in this time is measured as a metric. As opposed to Fixed-Work Benchmarks the Fixed-Time Benchmarks do not suffer the issue of an upper-bounded maximum improvement as for every improvement, the reduced time can be used to process further work units (See \cite{Kounev} p. 11).

Additionally there are also further approaches that does not employ fixed time or work at all, but rather employ variable-time and -work characteristics. For this case the matric is a function of the time $T$ and the work $W$. While this technique provides maximum flexibility, this approach is not suitable for all types of benchmarks (See \cite{Kounev} p. 12)

\subsection{Evaluating Benchmarks}

Apart from the metrics that evaluate the result of a benchmark, there are also several attributes that are important to evaluate the quality of the benchmark itself. The designer of a benchmark should always take these attributes into consideration when creating a benchmark. As described by Skadron et al. it is not possible to create a single benchmark that covers all these attributes perfectly. Therefore one either has to create mutliple benchmarks that complement each others weaknesses or choose to sacrifice one of these attributes over the other (\cite{Skadron2003} p. 32f).

Over the years, researchers and industry associates alike have defined several criteria that are considered desirable by modern standards. Often the exact terminology of these criteria differs based on authors and the specific application domain, however the concepts in general are similar. Kounev at al. list five core criteria that will be defined in a little more detail in this chapter (\cite{Kounev} p. 13f). A much more in depth discussion of these criteria was performed by Kistowski et al. (See \cite{kistowski2015})

\paragraph{Relevance} TODO: Short explanation
\paragraph{Reproducibility} TODO: Short explanation
\paragraph{Fairness} TODO: Short explanation
\paragraph{Verifiability} TODO: Short explanation
\paragraph{Usability} TODO: Short explanation

\section{Detecting Performance Changes}

\section{Continuous Benchmarking}
\subsection{Definition}
\subsection{Usages}
\subsection{Challenges}

\section{Experiment}


	\subsection{Experiment Idea \& Goals}
	\subsection{Experiment Setup}
	\subsection{Experiment Results}

\section{Evaluation of Performance Results}

\section{Reflection}

\section{Future Work}
	
	%*********************************************************************%
	% APPENDIX                                                            %
	%*********************************************************************%
	
	% Insert the appendix here. You can alternatively include files via: \include{pathToFile}
	
	%*********************************************************************%
	% LITERATURE                                                          %
	%*********************************************************************%
	% As a recommendation JabRef might be a usefull tool for this section. Use myRefs.bib therefore
	\phantomsection
	\bibliographystyle{splncs03}
	\bibliography{literature}
\end{document}
