\documentclass[	runningheads,
				%deutsch, % Tell llncs that the keywords should be in german
				%german,  % Needed for the \ifgerman-command
				a4paper]{llncs}

\usepackage{url}
\usepackage{graphicx}
\usepackage{amssymb}

% Support for special characters like "Umlaute"
\usepackage[utf8]{inputenc}

\ifgerman{
	\usepackage[ngerman]{babel}
	\usepackage{ngerman}
	\usepackage{bibgerm}        % German symbols in BibTeX
}{
	\usepackage[english]{babel}
}

%*********************************************************************%
% META                                                                %
%*********************************************************************%
\newcommand{\university}{Saarland University}
\newcommand{\school}{Saarland Informatics Campus}
\newcommand{\thetitle}{Seminar: Systems Benchmarking}
\newcommand{\shorttitle}{Seminar: Systems Benchmarking}
\newcommand{\thedate}{April 15}
\newcommand{\thegermandate}{15. April}

\newcommand{\theforename}{Lukas}
\newcommand{\thesurname}{Abelt}

% Advisors
	\newcommand{\advisor}{Advisors}
	\newcommand{\advisors}{Prof. Sven Apel, \\ Christian Hechtl}

% Title for the seminar
	%\newcommand{\theseminartitle}{"Performance Measurements Before Releases vs. Each Commits"}
	\newcommand{\theseminartitle}{"Using Continuous Benchmarking in Productive Systems -- Opportunities and Challenges"}

%*********************************************************************%
% THE DOCUMENT                                                        %
%*********************************************************************%

\begin{document}
	%*********************************************************************%
	% TITLE                                                               %
	%*********************************************************************%
	
	% Arabic page numbering
	\mainmatter 
		
	% Title including a subtitle for the name of the seminar
	\title{\theseminartitle \\ \small \thetitle}
	
	% (Optional) In the case that the initial title is too long, the short title will be used
	%\titlerunning{Hauptseminar: Human and Social Factors in Software Engineering}
	
	\author{\theforename\ \thesurname \small \\ \ \\ \advisor : \ \advisors}
	
	% (Optional) This will appear near the page number
	\authorrunning{\shorttitle}
	
	\institute{\school ,\\ \university}
	
	\maketitle
	
	%*********************************************************************%
	% CONTENT                                                             %
	%*********************************************************************%

% General Structure
	% 1. Introduction and Motivation
	% 2. General Definitions
	%		2.1 What is a Benchmark
	%		2.2 What is a continuous Benchmark
	% 3. Experiment
	%		3.1 Experiment Idea&Goals
	%		3.2 Setup
	%		3.3 Evaluation
	%	4. 

	% Introduction
	\section{General}
	This document will briefly outline the experiment idea for the topic "Performance Measurements Before Releases vs. Each Commits" in the seminar "Systems Benchmarking". The goal of this experiment and the corresponding paper is to showcase, how continous performance measurements throughout the whole development process can be used to detect and fix performance regressions at a very early stage.

	To achieve this, the experiment will first analyze the performance of public releases an open-source project to find a performance regression between two releases. Then, a performance measurement will be done for every commit between these two releases. We will use the data aquired by these performance measurements to demonstrate how or if various change detection measurements could havedetected this regression early on.

\section{Experiment Setup}

For my experiment I will use the open-source compression library \texttt{lz4}\footnote{\url{https://github.com/lz4/lz4}} as it has a active commit and release history. For the performance measurement itself we will use the latest version of the open-source project \texttt{lzbench}\footnote{https://github.com/inikep/lzbench}. Lzbench is an in-memory compression benchmarking tool that is originally designed to compare different compression algorithms. However for our case we will use it to simply only measure \texttt{lz4} at different releases/commits. For this case we will disable all other compression algorithms that \texttt{lzbench} supports at compile-time. First preliminary tests have shown that \texttt{lzbench} can easily use a different version of \texttt{lz4} than the shipped one by simply replacing the corresponding source files.

For the compression itself we will use the Silesia Data Corpus\footnote{http://sun.aei.polsl.pl/~sdeor/index.php?page=silesia}. The Sileasia Corpus combines a variety of different data in order to create a diverse dataset for compression measurements. 

After identifying a performance regression between two releases we will collect data for the commits between these releases. To analyze the data collected for these commits we will compare the two following methods:
\begin{itemize}
	\item Comparing based on a static threshhold to the last commit/release
	\item Using Change Point Detection (Specifically E-Divisive Means)\footnote{As described in \url{https://dl.acm.org/doi/pdf/10.1145/3358960.3375791}}
\end{itemize}

For the change point detection we will use the implementation provided by MongoDB\footnote{https://github.com/mongodb/signal-processing-algorithms}. Since we will also store the raw performance data as returned by \texttt{lzbench} it is also possible to add further methods later on to get a better overview of different methods.

\section{Challenges and possible risks}
In this section I will briefly outline challenges, risks that might arise during the experiment and also outline possible solutions to them.

\subsection{Detecting regression between releases}
\subsubsection{Defining a regression threshhold}
One of the main challenges will be to define a threshhold for a performance regression between commits. As a first starter we will work with a static threshhold of 10\% as compared to the previous release. However if this value seems to many/less releases we might need to change the threshhold or overthink out strategy overall.

\subsubsection{No regression detected}
It might happen that we will detect no noticable performance changes whatsoever between the individual releases. One likely reason for this might be that the actual performance regressions happened while testing the release and where therefore fixed in the actual release.

One solution to this might be to select some commit for measurement before the release commit to benchmark on. However this would require a lot of manual triaging to select the "correct" commit based on e.g. the commit or issue history of the project.

Then still it could happen that with that strategy we do not detect any performance regressions (or improvements) that we can use as a guideline where to perform our finer measurement. In that case we could:
\begin{itemize}
	\item Select an arbitrary release range for our finer analysis -- However this itself bears the risk that we do not detect any change points in this range
	\item Select a release range based on issues submitted on the GitHub page that indicate that there at least was some performance regression during this period
	\item Select another project for our experiment
\end{itemize}

\subsection{Measuring individual commits}

\subsubsection{Commits that won't build}
If we measure all commits between two releases it is very likely that not all commits will be able to compile. We can easily take this into account by simply skipping these commits and not perform any measurements if we detect a failure during build.

\subsubsection{Number of commits}
Even when restricting us to the duration between two releases it might still happen that we end up with a lot of commits we need to benchmark individually. Depending how long one benchmark takes this could easily take too much time to finish in a reasonable time frame. Additionally we have to remember that also others might want to use the cluster environment for their experiments, so we should not clog it with hundreds of our jobs.

One possible measure to counteract this is to not measure each individual commit but squash them based on some criterion, e.g. only measuring one commit per day. In practice we will have to take a close look how long an individual performance measurement (including build etc.) takes so that we can esitmate how long our measurements would take in total.

\section{Possible Extensions}
There are a few possible extensions that could be appended to this experiment idea if time allows it:
\begin{itemize}
	\item Using the data collected by individual commits to show how individual commits can be compared effectively\footnote{As shown in \url{https://arxiv.org/pdf/2101.10231.pdf}}
	\item Extend the regression detection method
	\item Evaluate the impact on sustainability
\end{itemize}
	
	%*********************************************************************%
	% APPENDIX                                                            %
	%*********************************************************************%
	
	% Insert the appendix here. You can alternatively include files via: \include{pathToFile}
	
	%*********************************************************************%
	% LITERATURE                                                          %
	%*********************************************************************%
	% As a recommendation JabRef might be a usefull tool for this section. Use myRefs.bib therefore
	
\end{document}
